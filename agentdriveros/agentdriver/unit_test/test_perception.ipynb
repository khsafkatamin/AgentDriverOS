{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**A Language Agent for Autonomous Driving**\n",
      "Role: You are the brain of an autonomous vehicle (a.k.a. ego-vehicle). In this step, you need to extract necessary information from the driving scenario. The information you extracted must be useful to the next-step motion planning. \n",
      "\n",
      "Necessary information might include the following:\n",
      "- Detections: The detected objects that you need to pay attention to.\n",
      "- Predictions: The estimated future motions of the detected objects. \n",
      "- Maps: Map information includes traffic lanes and road boundaries.\n",
      "- Occunpancy: Occupancy implies whether a location has been occupied by other objects.\n",
      "\n",
      "Task\n",
      "- You should think about what types of information (Detections, Predictions, Maps, Occupancy) you need to extract from the driving scenario.\n",
      "- Detections and Predictions are quite important for motion planning. You should call at least one of them if necessary.\n",
      "- Maps information are also important. You should pay more attention to road shoulder and lane divider information to your current ego-vehicle location.\n",
      "- I will guide you through the thinking process step by step.\n",
      "\n",
      "*****Ego States:*****\n",
      "Current State:\n",
      " - Velocity (vx,vy): (-0.01,0.92)\n",
      " - Heading Angular Velocity (v_yaw): (0.00)\n",
      " - Acceleration (ax,ay): (-0.00,-0.50)\n",
      " - Can Bus: (-0.74,0.14)\n",
      " - Heading Speed: (0.95)\n",
      " - Steering: (-0.02)\n",
      "Historical Trajectory (last 2 seconds): [(-0.07,-6.43), (-0.05,-4.34), (-0.02,-2.32), (-0.01,-0.91)]\n",
      "Mission Goal: FORWARD\n",
      "\n",
      "\n",
      "\n",
      "Do you need to perform detections from the driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "You can execute one of the following functions to get object detection results (don't execute functions that have been used before):\n",
      "- get_leading_object_detection() #Get the detection of the leading object, the function will return the leading object id and its position and size. If there is no leading object, return None\n",
      "- get_object_detections_in_range(x_start, x_end, y_start, y_end) #Get the detections of the objects in a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return a list of object ids and their positions and sizes. If there is no object, return None\n",
      "- get_surrounding_object_detections() #Get the detections of the surrounding objects in a 20m*20m range, the function will return a list of surroundind object ids and their positions and sizes. If there is no surrounding object, return None\n",
      "- get_front_object_detections() #Get the detections of the objects in front of you in a 10m*20m range, the function will return a list of front object ids and their positions and sizes. If there is no front object, return None\n",
      "- get_all_object_detections() #Get the detections of all objects in the whole scene, the function will return a list of object ids and their positions and sizes. Always avoid using this function if there are other choices.\n",
      "\n",
      "None\n",
      "get_front_object_detections\n",
      "{}\n",
      "Front object detections:\n",
      "Front object detected, object type: car, object id: 2, position: (4.36, 9.56), size: (1.86, 4.72)\n",
      "Front object detected, object type: car, object id: 3, position: (-3.70, 13.08), size: (2.01, 4.92)\n",
      "\n",
      "\n",
      "Do you need to perform future trajectory predictions for the detected objects?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "You can execute one of the following functions to get object future trajectory predictions (don't execute functions that have been used before):\n",
      "- get_leading_object_future_trajectory() #Get the predicted future trajectory of the leading object, the function will return a trajectory containing a series of waypoints. If there is no leading vehicle, return None\n",
      "- get_future_trajectories_for_specific_objects(object_ids) #Get the future trajectories of specific objects (specified by a List of object ids), the function will return trajectories for each object. If there is no object, return None\n",
      "- get_future_trajectories_in_range(x_start, x_end, y_start, y_end) #Get the future trajectories where any waypoint in this trajectory falls into a given range (x_start, x_end)*(y_start, y_end)m^2, the function will return each trajectory that satisfies the condition. If there is no trajectory satisfied, return None\n",
      "- get_future_waypoint_of_specific_objects_at_timestep(object_ids, timestep) #Get the future waypoints of specific objects at a specific timestep, the function will return a list of waypoints. If there is no object or the object does not have a waypoint at the given timestep, return None\n",
      "- get_all_future_trajectories() #Get the predicted future trajectories of all objects in the whole scene, the function will return a list of object ids and their future trajectories. Always avoid using this function if there are other choices.\n",
      "\n",
      "None\n",
      "get_future_trajectories_for_specific_objects\n",
      "{'object_ids': [2, 3]}\n",
      "Future trajectories for specific objects:\n",
      "Object type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\n",
      "Object type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\n",
      "\n",
      "\n",
      "Do you need to get occupancy information for this driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "NO\n",
      "\n",
      "Do you need to get map information for this driving scenario?\n",
      "Please answer YES or NO.\n",
      "\n",
      "YES\n",
      "You can execute one of the following functions to get map information (don't execute functions that have been used before):\n",
      "- get_drivable_at_locations(locations) #Get the drivability at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_lane_category_at_locations(locations, return_score) #Get the lane category at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_distance_to_shoulder_at_locations(locations) #Get the distance to both sides of road shoulders at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_shoulder() #Get the distance to both sides of road shoulders for the current ego-vehicle location.\n",
      "- get_distance_to_lane_divider_at_locations(locations) #Get the distance to both sides of road lane_dividers at the locations [(x_1, y_1), ..., (x_n, y_n)]. If the location is out of the map scope, return None\n",
      "- get_current_lane_divider() #Get the distance to both sides of road lane_dividers for the current ego-vehicle location\n",
      "- get_nearest_pedestrian_crossing() #Get the location of the nearest pedestrian crossing to the ego-vehicle. If there is no such pedestrian crossing, return None\n",
      "\n",
      "None\n",
      "get_current_shoulder\n",
      "{}\n",
      "Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "Current ego-vehicle's distance to left shoulder is 7.5m and right shoulder is 4.0m\n",
      "\n",
      "*****Perception Results:*****\n",
      "Front object detections:\n",
      "Front object detected, object type: car, object id: 2, position: (4.36, 9.56), size: (1.86, 4.72)\n",
      "Front object detected, object type: car, object id: 3, position: (-3.70, 13.08), size: (2.01, 4.92)\n",
      "\n",
      "Future trajectories for specific objects:\n",
      "Object type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\n",
      "Object type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\n",
      "\n",
      "Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "Current ego-vehicle's distance to left shoulder is 7.5m and right shoulder is 4.0m\n",
      "\n",
      "\n",
      "*****Perception Results:*****\n",
      "Front object detections:\n",
      "Front object detected, object type: car, object id: 2, position: (4.36, 9.56), size: (1.86, 4.72)\n",
      "Front object detected, object type: car, object id: 3, position: (-3.70, 13.08), size: (2.01, 4.92)\n",
      "\n",
      "Future trajectories for specific objects:\n",
      "Object type: car, object id: 2, future waypoint coordinates in 3s: [(4.36, 9.56), (4.36, 9.56), (4.36, 9.57), (4.36, 9.57), (4.36, 9.56), (4.36, 9.56)]\n",
      "Object type: car, object id: 3, future waypoint coordinates in 3s: [(-2.66, 13.82), (-1.69, 14.79), (-0.99, 16.13), (-0.25, 17.73), (0.19, 19.42), (0.57, 21.35)]\n",
      "\n",
      "Distance to both sides of road shoulders of current ego-vehicle location:\n",
      "Current ego-vehicle's distance to left shoulder is 7.5m and right shoulder is 4.0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from agentdriver.perception.perception_agent import PerceptionAgent\n",
    "from agentdriver.llm_core.api_keys import OPENAI_ORG, OPENAI_API_KEY\n",
    "\n",
    "import openai\n",
    "openai.organization = OPENAI_ORG\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('../../data/')\n",
    "\n",
    "perception_agent = PerceptionAgent(token=\"0a0d6b8c2e884134a3b48df43d54c36a\", split=\"val\", data_path=data_path, verbose=True)\n",
    "\n",
    "ego_prompts, perception_prompts, working_memory = perception_agent.run()\n",
    "\n",
    "print(perception_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
